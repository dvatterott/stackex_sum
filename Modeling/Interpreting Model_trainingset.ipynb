{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## create sql table ################\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "dbname = 'stack_exchange_rnn_db'\n",
    "q_tbname = 'question_table'\n",
    "a_tbname = 'answer_table'\n",
    "username = 'dan-laptop'\n",
    "import os\n",
    "password = os.environ['PGRES_PASSWORD']\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost:5432/%s'%(username,password,dbname))\n",
    "\n",
    "## Now access sql db from python\n",
    "con = None\n",
    "connect_str = \"dbname='%s' user='%s' host='localhost' password='%s'\"%(dbname,username,password)\n",
    "con = psycopg2.connect(connect_str)\n",
    "cur = con.cursor() #create cursor for communicating with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_vec</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the,error,object,may,read,from,the,network,net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this,is,what,i,did,worked,for,me,when,reading,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>possible,duplicate,stackoverflow,com,questions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you,are,looking,for,idxmax,in,1332,x,out,1332,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x,max,x,max,x,max,axis,1,max,index,this,works,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word_vec  score\n",
       "0  the,error,object,may,read,from,the,network,net...      0\n",
       "1  this,is,what,i,did,worked,for,me,when,reading,...      0\n",
       "2  possible,duplicate,stackoverflow,com,questions...      0\n",
       "3  you,are,looking,for,idxmax,in,1332,x,out,1332,...      2\n",
       "4  x,max,x,max,x,max,axis,1,max,index,this,works,...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "################# make query ########################\n",
    "sql_query = \"\"\"\n",
    "    SELECT answer_table.word_vec, answer_table.score \n",
    "    FROM answer_table\n",
    "    INNER JOIN question_table\n",
    "        on answer_table.q_id = question_table.q_id\n",
    "        and question_table.view_count > 50\n",
    "    LIMIT 10000;\n",
    "\"\"\"\n",
    "question_df = pd.read_sql_query(sql_query,con)\n",
    "question_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gimme that overflow!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.layers import Dense, Input, GRU, Embedding\n",
    "from keras.models import Model\n",
    "import six.moves.cPickle as cPickle\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300\n",
    "word_index_length = 374000\n",
    "\n",
    "embedding_layer = Embedding(word_index_length + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(128, dropout_W=0.2, dropout_U=0.2)(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "mymodel = Model(sequence_input, preds)\n",
    "mymodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "mymodel.load_weights('../Data_and_Models/stackex_gru.h5')\n",
    "print('Gimme that overflow!')\n",
    "vectorizer = cPickle.load(open('../Data_and_Models/rnn_tokenizer.pkl', 'rb'), encoding='latin1')\n",
    "lg = mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fetch_and_clean(text,model_prep=False):\n",
    "    raw = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    raw = raw.lower()\n",
    "\n",
    "    if model_prep == False:\n",
    "        tokens = text_to_word_sequence(raw)\n",
    "    elif model_prep == True:\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        tokens = [word for word in tokens if not word.isdigit()]\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyRNN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "\n",
    "    def predict(self, a_vect):\n",
    "        just_tok = [fetch_and_clean(x,model_prep=False) for x in a_vect]\n",
    "        just_tok = [vectorizer.texts_to_sequences(x) for x in just_tok]\n",
    "        temp = []\n",
    "        for items in just_tok: temp.append([x[0] for x in items if len(x)>0])\n",
    "        just_tok = temp\n",
    "\n",
    "        padded_seq = pad_sequences(just_tok,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        seq = np.array(padded_seq)\n",
    "        \n",
    "        return self.lg.predict(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RNN = MyRNN(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the,error,object,may,read,from,the,network,network,is,not,seekable,you,can't,go,back,in,the,general,case,you,could,replace,err,with,a,new,httperror,instance,that,reads,from,a,buffer,like,io,bytesio,instead,of,the,network,e,g,not,tested,content,err,read,self,log,exception,content,raise,httperror,err,url,err,code,err,reason,err,headers,io,bytesio,content,though,i'm,not,sure,that,you,should,handle,the,error,in,a,single,place,instead,e,g,reraise,a,more,application,specific,exception,or,leave,the,logging,to,an,upstream,handler\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_df.ix[0]['word_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67328143,  0.32671857]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN.predict([question_df.ix[0]['word_vec']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Lime to see what my model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "import pickle\n",
    "\n",
    "class_names = ['unhelpful', 'helpful']\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "RNN = MyRNN(lg)\n",
    "new = False\n",
    "\n",
    "if new == True:\n",
    "    word_dict = {}\n",
    "row_gen = question_df['word_vec'][1500:].iteritems()\n",
    "\n",
    "name = 'lime_dict_train'\n",
    "with open('../Data_and_Models/' + name + '.pkl', 'rb') as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "for count,items in enumerate(row_gen):\n",
    "    exp = explainer.explain_instance(items[1], RNN.predict, num_features=6)\n",
    "    word_list = exp.as_list()\n",
    "    for words in word_list:\n",
    "        if words[0] in word_dict.keys():\n",
    "            word_dict[words[0]] = np.append(word_dict[words[0]],words[1])\n",
    "        else:\n",
    "            word_dict[words[0]] = words[1]\n",
    "    if count == 600: break\n",
    "\n",
    "#check out the results. \n",
    "import pickle\n",
    "\n",
    "name = 'lime_dict_train'\n",
    "with open('../Data_and_Models/'+ name + '.pkl', 'wb') as f:\n",
    "    pickle.dump(word_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "import numpy as np\n",
    "num_wanted = 20\n",
    "imp_word_dict = {}\n",
    "\n",
    "for i in range(num_wanted):\n",
    "    #this is convoluted because words that have only been seen once are irratic\n",
    "    max_key = max(word_dict.keys(), key=(lambda x: 0 if isinstance(word_dict[x], float) else abs(np.mean(word_dict[x]))))\n",
    "    imp_word_dict[max_key] = np.mean(word_dict[max_key])\n",
    "    word_dict.pop(max_key, None)\n",
    "    \n",
    "with open('../Data_and_Models/' + name + '.pkl', 'rb') as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
