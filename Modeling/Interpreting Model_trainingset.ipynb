{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## create sql table ################\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "\n",
    "dbname = 'stack_exchange_rnn_db'\n",
    "q_tbname = 'question_table'\n",
    "a_tbname = 'answer_table'\n",
    "username = 'dan-laptop'\n",
    "import os\n",
    "password = os.environ['PGRES_PASSWORD']\n",
    "\n",
    "engine = create_engine('postgresql://%s:%s@localhost:5432/%s'%(username,password,dbname))\n",
    "\n",
    "## Now access sql db from python\n",
    "con = None\n",
    "connect_str = \"dbname='%s' user='%s' host='localhost' password='%s'\"%(dbname,username,password)\n",
    "con = psycopg2.connect(connect_str)\n",
    "cur = con.cursor() #create cursor for communicating with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_vec</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the,error,object,may,read,from,the,network,net...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this,is,what,i,did,worked,for,me,when,reading,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>possible,duplicate,stackoverflow,com,questions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you,are,looking,for,idxmax,in,1332,x,out,1332,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x,max,x,max,x,max,axis,1,max,index,this,works,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word_vec  score\n",
       "0  the,error,object,may,read,from,the,network,net...      0\n",
       "1  this,is,what,i,did,worked,for,me,when,reading,...      0\n",
       "2  possible,duplicate,stackoverflow,com,questions...      0\n",
       "3  you,are,looking,for,idxmax,in,1332,x,out,1332,...      2\n",
       "4  x,max,x,max,x,max,axis,1,max,index,this,works,...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "################# make query ########################\n",
    "sql_query = \"\"\"\n",
    "    SELECT answer_table.word_vec, answer_table.score \n",
    "    FROM answer_table\n",
    "    INNER JOIN question_table\n",
    "        on answer_table.q_id = question_table.q_id\n",
    "        and question_table.view_count > 50\n",
    "    LIMIT 10000;\n",
    "\"\"\"\n",
    "question_df = pd.read_sql_query(sql_query,con)\n",
    "question_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gimme that overflow!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.layers import Dense, Input, GRU, Embedding\n",
    "from keras.models import Model\n",
    "import six.moves.cPickle as cPickle\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300\n",
    "word_index_length = 374000\n",
    "\n",
    "embedding_layer = Embedding(word_index_length + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = GRU(128, dropout_W=0.2, dropout_U=0.2)(embedded_sequences)\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "mymodel = Model(sequence_input, preds)\n",
    "mymodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "mymodel.load_weights('../Data_and_Models/stackex_gru.h5')\n",
    "print('Gimme that overflow!')\n",
    "vectorizer = cPickle.load(open('../Data_and_Models/rnn_tokenizer.pkl', 'rb'), encoding='latin1')\n",
    "lg = mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fetch_and_clean(text,model_prep=False):\n",
    "    raw = BeautifulSoup(text, \"lxml\").get_text()\n",
    "    raw = raw.lower()\n",
    "\n",
    "    if model_prep == False:\n",
    "        tokens = text_to_word_sequence(raw)\n",
    "    elif model_prep == True:\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        tokens = [word for word in tokens if not word.isdigit()]\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyRNN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lg):\n",
    "        self.lg = lg\n",
    "\n",
    "    def predict(self, a_vect):\n",
    "        just_tok = [fetch_and_clean(x,model_prep=False) for x in a_vect]\n",
    "        just_tok = [vectorizer.texts_to_sequences(x) for x in just_tok]\n",
    "        temp = []\n",
    "        for items in just_tok: temp.append([x[0] for x in items if len(x)>0])\n",
    "        just_tok = temp\n",
    "\n",
    "        padded_seq = pad_sequences(just_tok,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        seq = np.array(padded_seq)\n",
    "        \n",
    "        return self.lg.predict(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RNN = MyRNN(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the,error,object,may,read,from,the,network,network,is,not,seekable,you,can't,go,back,in,the,general,case,you,could,replace,err,with,a,new,httperror,instance,that,reads,from,a,buffer,like,io,bytesio,instead,of,the,network,e,g,not,tested,content,err,read,self,log,exception,content,raise,httperror,err,url,err,code,err,reason,err,headers,io,bytesio,content,though,i'm,not,sure,that,you,should,handle,the,error,in,a,single,place,instead,e,g,reraise,a,more,application,specific,exception,or,leave,the,logging,to,an,upstream,handler\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_df.ix[0]['word_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67328143,  0.32671857]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN.predict([question_df.ix[0]['word_vec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class_names = ['unhelpful', 'helpful']\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "RNN = MyRNN(lg)\n",
    "new = False\n",
    "\n",
    "if new == True:\n",
    "    word_dict = {}\n",
    "    row_gen = question_df['word_vec'].iteritems()\n",
    "\n",
    "for count,items in enumerate(row_gen):\n",
    "    exp = explainer.explain_instance(items[1], RNN.predict, num_features=6)\n",
    "    word_list = exp.as_list()\n",
    "    for words in word_list:\n",
    "        if words[0] in word_dict.keys():\n",
    "            word_dict[words[0]] = np.append(word_dict[words[0]],words[1])\n",
    "        else:\n",
    "            word_dict[words[0]] = words[1]\n",
    "    if count == 600: break\n",
    "\n",
    "#check out the results. \n",
    "import pickle\n",
    "\n",
    "name = 'lime_dict_train'\n",
    "with open('./'+ name + '.pkl', 'wb') as f:\n",
    "    pickle.dump(word_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "import numpy as np\n",
    "num_wanted = 20\n",
    "imp_word_dict = {}\n",
    "\n",
    "for i in range(num_wanted):\n",
    "    #this is convoluted because words that have only been seen once are irratic\n",
    "    max_key = max(word_dict.keys(), key=(lambda x: 0 if isinstance(word_dict[x], float) else abs(np.mean(word_dict[x]))))\n",
    "    imp_word_dict[max_key] = np.mean(word_dict[max_key])\n",
    "    word_dict.pop(max_key, None)\n",
    "    \n",
    "with open('./' + name + '.pkl', 'rb') as f:\n",
    "    word_dict = pickle.load(f)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./' + name + '.pkl', 'rb') as f:\n",
    "    word_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "sorted_x = sorted(imp_word_dict, key=imp_word_dict.get)\n",
    "sorted_y = [imp_word_dict[x] for x in sorted_x]\n",
    "#sorted_y.reverse()\n",
    "#sorted_x.reverse()\n",
    "\n",
    "plt.barh(range(len(sorted_x)), sorted_y, width, color='r')\n",
    "plt.yticks(range(len(sorted_x)), sorted_x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0158031820110008"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@hulk: you seem to be under the impression that type is the right way to do this.',\n",
       " 'it is (almost certainly) not.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical': 0.033773051465441559,\n",
       " 'check': -0.071724063087750436,\n",
       " 'duplicate': 0.039175234219797961,\n",
       " 'possible': -0.024375775445865035,\n",
       " 'python': 0.023019418089235359,\n",
       " 's': 0.026437705602720015,\n",
       " 'seem': 0.023015186501761417,\n",
       " 'that': -0.0129267123790195,\n",
       " 'this': -0.036796228488811485,\n",
       " 'to': -0.01281247016068549,\n",
       " 'way': 0.01404254601200421,\n",
       " 'you': 0.056372161126580327}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking out what the data lime wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'From: johnchad@triton.unm.edu (jchadwic)\\nSubject: Another request for Darwin Fish\\nOrganization: University of New Mexico, Albuquerque\\nLines: 11\\nNNTP-Posting-Host: triton.unm.edu\\n\\nHello Gang,\\n\\nThere have been some notes recently asking where to obtain the DARWIN fish.\\nThis is the same question I have and I have not seen an answer on the\\nnet. If anyone has a contact please post on the net or email me.\\n\\nThanks,\\n\\njohn chadwick\\njohnchad@triton.unm.edu\\nor\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#http://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']\n",
    "newsgroups_test.data[83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29  0.71]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 83\n",
      "Probability(christian) = 0.446\n",
      "True class: unhelpful\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from __future__ import print_function\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train_vectors, newsgroups_train.target)\n",
    "\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, rf)\n",
    "\n",
    "print(c.predict_proba([newsgroups_test.data[0]]))\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "idx = 83\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(christian) =', c.predict_proba([newsgroups_test.data[idx]])[0,1])\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
