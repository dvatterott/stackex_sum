{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./Data_and_Models/full_answers_50more_views.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/site-packages/ipykernel/__main__.py:27: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'word_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fb0c1457873a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mmy_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data_and_Models/full_answers_50more_views.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-fb0c1457873a>\u001b[0m in \u001b[0;36mgetData\u001b[0;34m(filename, batch_size, threshold, balance)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mpadded_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    129\u001b[0m         '''\n\u001b[1;32m    130\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dan-laptop/anaconda3/envs/insight/lib/python3.5/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnb_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnb_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'word_index'"
     ]
    }
   ],
   "source": [
    "import csv, copy, codecs\n",
    "import numpy as np\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "def getData(filename,batch_size=4,threshold=2,balance=False):\n",
    "    with open(filename, \"rb\") as csv_file:\n",
    "        csv_obj = csv.reader(codecs.iterdecode(csv_file, 'utf-8'))\n",
    "        next(csv_obj, None)  # skip the headers\n",
    "        count = 0\n",
    "        for row in csv_obj:\n",
    "            if count == 0: list_data = []\n",
    "\n",
    "            list_data.append(row)  \n",
    "            count += 1\n",
    "        \n",
    "            if count == batch_size:\n",
    "                df = pd.DataFrame(list_data, columns=['index','word_vec','score'])\n",
    "                df = df[df['word_vec'].notnull()]\n",
    "                \n",
    "                df['score'] = df['score'].convert_objects(convert_numeric=True)\n",
    "                y = df['score'].copy()\n",
    "                y[y<threshold] = 0\n",
    "                y[y>0] = 1\n",
    "                \n",
    "                data = []\n",
    "                gen = df.iterrows()\n",
    "                for i,rows in enumerate(gen):\n",
    "                    tokens = tokenizer.texts_to_sequences(rows[1]['word_vec'].split(','))\n",
    "                    tokens = [x[0] for x in tokens if len(x)>0]\n",
    "                    padded_seq = pad_sequences([tokens],maxlen=MAX_SEQUENCE_LENGTH)\n",
    "                    data.append(padded_seq)\n",
    "\n",
    "                data = np.array(data)\n",
    "                data = np.squeeze(data)\n",
    "                \n",
    "                if balance == True:\n",
    "                    how_many_y = np.sum(y==1) #this is going to be random batchsizes...\n",
    "                    new_y = copy.copy(y[y==1][:how_many_y]) #more rare\n",
    "                    new_y = np.append(new_y,y[y==0][:how_many_y])\n",
    "\n",
    "                    new_x = copy.copy(data[y==1,:][:how_many_y])\n",
    "                    new_x = np.append(new_x,data[y==0,:][:how_many_y],axis=0)\n",
    "                else:\n",
    "                    new_x = data\n",
    "                    new_y = y\n",
    "                \n",
    "                indices = np.arange(new_x.shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                new_x = new_x[indices]\n",
    "                new_y = new_y[indices]\n",
    "                \n",
    "                bal_y = to_categorical(bal_y)\n",
    "                \n",
    "                yield (x,y)\n",
    "                count = 0\n",
    "\n",
    "my_gen = getData('../Data_and_Models/full_answers_50more_views.csv')\n",
    "next(my_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['word_vec'].notnull()]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "threshold = 2\n",
    "\n",
    "y = df['score'].copy()\n",
    "y[y<threshold] = 0\n",
    "y[y>0] = 1\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "bal_y = copy.copy(y[y==1][:nb_samples_to_use/2])\n",
    "bal_y = np.append(bal_y,y[y==0][:nb_samples_to_use/2])\n",
    "\n",
    "bal_x = copy.copy(data[y==1,:][:nb_samples_to_use/2])\n",
    "bal_x = np.append(bal_x,data[y==0,:][:nb_samples_to_use/2],axis=0)\n",
    "print(bal_y.shape)\n",
    "print(bal_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = []\n",
    "gen = df.iterrows()\n",
    "for i,rows in enumerate(gen):\n",
    "    tokens = tokenizer.texts_to_sequences(rows[1]['word_vec'].split(','))\n",
    "    tokens = [x[0] for x in tokens if len(x)>0]\n",
    "    padded_seq = pad_sequences([tokens],maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    data.append(padded_seq)\n",
    "\n",
    "data = np.array(data)\n",
    "data = np.squeeze(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "bal_y = to_categorical(bal_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
